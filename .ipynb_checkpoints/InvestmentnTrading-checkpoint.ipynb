{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Forecaster 0.1b\n",
    "## Machine Learning Engineer Nanodegree\n",
    "**Calvin Ku**\n",
    "\n",
    "**June 6, 2016**\n",
    "\n",
    "## Definition\n",
    "\n",
    "### Project Overview\n",
    "This project is about building a stock price forecaster. The forecaster is generally a regressor, which tries to predict a number for a given stock for a given future date (for example, the price of GOOGL 10 days from now). The goal of this project is to build the forecaster that can predict the stock price 63 days from the day the prediction is made. We will use GOOGL as an example in this project but the same method can be applied to any stock. The forecaster is meant be combined with a portfolio optimizer to form a total decisioning solution for trading to give trading suggustion to investors..\n",
    "\n",
    "#### Data used in this project\n",
    "The datasets that will be used in this project include:\n",
    "* Historical stock prices data\n",
    "* St. Louis Fed Financial Stress Index data\n",
    "\n",
    "For the historical data, we will ready the stocks of S&P 500 (2009) range from January 1, 2009, to June 28, 2016. Some of the stocks are not traded as of this writring so are excluded. \n",
    "\n",
    "### Problem Statement\n",
    "Problem with trading is that you never know when is the best time to buy or sell a stock, as you never know if the stock price will go up or go down in the future. This simple forecaster is an attempt to solve this problem.\n",
    "\n",
    "For any stock in S&P 500 (2009), the forecaster is able to predict the prices of that stock 63 days in the future. The precision of the forecast vary from stock to stock but for the forecaster to be anyway useful generally we want to limit the mean error to be around 5% of the stock price.\n",
    "\n",
    "### Metrics\n",
    "In this project we use MSE (mean sqauared error) as the metric, since our goal is to try to make the predicted price as close to real price as possible. To be more specific, the reason why MSE is good is because:\n",
    "\n",
    "1. The overestimations (when the predicted value is higher than the actual) and underestimations (when lower than the actual) of the model don't cancel out when you take the average, therefore you won't underestimate the error.\n",
    "\n",
    "2. The square of a very small value is even smaller and the square of a big value is even bigger. We are looking for a model that can consistently make predictions that stay close to the actual, and with MSE, we can punish really off predictions even more (for example, being off by 20 is a lot more than twice as bad as being off by 10) to get a consistent model.\n",
    "\n",
    "3. One problem with MSE is that it is more susceptible to outliers. Fortunately, the good thing about financial data is that the data is quite pristine so we can expect not so much corrupted and misinput data. On the other hand, unlike cross-sectional data, the continuous nature of stock price data itself eliminates the possibility of any \"special case\" and makes spotting corrupted data rather easy.\n",
    "\n",
    "Along with MSE I also log the $r^2$ to get an idea of the correlation between the data and our model.\n",
    "\n",
    "## Analysis\n",
    "### Data Exploration\n",
    "About the data used in this project, we assume the following:\n",
    "\n",
    "* CAPM (Capital Asset Pricing Model), where it states that any stock price on the market is a multitude of the market trend (in our case we use the stock price of SPY), plus a constant $\\alpha$ which is specific to that stock. \n",
    "* Believing that economy is corrlated to the the stock market\n",
    "* Believing that any price change without a good amount of volume behind is just a random fluctuation\n",
    "\n",
    "Therefore, the raw data used in this project includes the following:\n",
    "\n",
    "* SPY Adj Close\n",
    "* SPY Volume\n",
    "* Adj Close of the target stock\n",
    "* Volume of the target stock\n",
    "* STLFSI - St. Louis Fed Financial Stress Index data\n",
    "\n",
    "Since SPY is always traded in the trading days, we use it to get the trading days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First look\n",
    "Let's first take a glance at our data and see if there's any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY_Vol</th>\n",
       "      <th>SPY</th>\n",
       "      <th>BF-B_Vol</th>\n",
       "      <th>BF-B</th>\n",
       "      <th>STLFSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>227566300.0</td>\n",
       "      <td>79.602650</td>\n",
       "      <td>562300</td>\n",
       "      <td>30.514303</td>\n",
       "      <td>3.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>240349700.0</td>\n",
       "      <td>79.508455</td>\n",
       "      <td>671200</td>\n",
       "      <td>30.600990</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>328260900.0</td>\n",
       "      <td>80.039370</td>\n",
       "      <td>623500</td>\n",
       "      <td>29.728326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>280899200.0</td>\n",
       "      <td>77.641697</td>\n",
       "      <td>765600</td>\n",
       "      <td>28.485795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-08</th>\n",
       "      <td>263834400.0</td>\n",
       "      <td>77.958535</td>\n",
       "      <td>629700</td>\n",
       "      <td>28.705403</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SPY_Vol        SPY  BF-B_Vol       BF-B  STLFSI\n",
       "2009-01-02  227566300.0  79.602650    562300  30.514303   3.643\n",
       "2009-01-05  240349700.0  79.508455    671200  30.600990     NaN\n",
       "2009-01-06  328260900.0  80.039370    623500  29.728326     NaN\n",
       "2009-01-07  280899200.0  77.641697    765600  28.485795     NaN\n",
       "2009-01-08  263834400.0  77.958535    629700  28.705403     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY_Vol        0\n",
      "SPY            0\n",
      "BF-B_Vol       0\n",
      "BF-B           0\n",
      "STLFSI      1510\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display\n",
    "\n",
    "def load_data(stock_list_csv, market_csv, start_date, end_date, symbol_list=None, random_state=0, number_of_stocks=10):\n",
    "    \"\"\"The function does the following:\n",
    "    1. Load the list of stocks\n",
    "    2. Load the market data (SPY or other ETF data)\n",
    "    3. Load the economy data (STLFSI or other index data)\n",
    "    4. Randomly picks 10 stocks and load into a dictionary of dataframes if symbol_list is not provided,\n",
    "       otherwise load the stocks in the symbo_list instead\n",
    "    5. Return a dictionary of dataframes of stock data, with stock symbols as the keys\n",
    "    \"\"\"\n",
    "    # Set up the empty main dataframe using the defined data range\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    df_main = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Load SPY to get trading days\n",
    "    dfSPY = pd.read_csv(market_csv, index_col='Date', parse_dates=True, usecols=['Date', 'Adj Close', 'Volume'], na_values = ['nan'])\n",
    "    dfSPY = dfSPY.rename(columns={'Adj Close': 'SPY', 'Volume': 'SPY_Vol'})\n",
    "    \n",
    "    # Get SPY within the target date range\n",
    "    df_main = df_main.join(dfSPY)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df_main = df_main.dropna()\n",
    "    \n",
    "    # Load target stock list\n",
    "    dfSPY500_2009 = pd.read_csv(stock_list_csv, header=None, usecols = [1])\n",
    "    \n",
    "    # Ready the symbol list\n",
    "    if symbol_list is None:\n",
    "        np.random.seed(random_state)\n",
    "        symbol_list = np.random.choice(dfSPY500_2009[1].tolist(), number_of_stocks)\n",
    "        \n",
    "    # Load the FSI data\n",
    "    dfFSI = pd.read_csv('STLFSI.csv', index_col='DATE', parse_dates=True, na_values = ['nan'])\n",
    "    \n",
    "    # Load target stocks\n",
    "    result_dict = {}\n",
    "    \n",
    "    for symbol in symbol_list:\n",
    "        df_temp = pd.read_csv('stock_data/' + symbol + '.csv', index_col=\"Date\", parse_dates=True, usecols = ['Date', 'Volume', 'Adj Close'], na_values=['nan'])\n",
    "        df_temp = df_temp.rename(columns={'Volume': symbol + '_Vol', 'Adj Close': symbol})\n",
    "        df_main = df_main.join(df_temp, how='left')\n",
    "        df_main = df_main.join(dfFSI, how='left')\n",
    "        result_dict[symbol] = df_main\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "symbol = 'BF-B'\n",
    "test_dict = load_data('dev.csv', 'stock_data/SPY.csv', '2009-01-01', '2016-06-30', symbol_list=['BF-B'], random_state=0, number_of_stocks=10)\n",
    "df_main = test_dict[symbol]\n",
    "\n",
    "# # Define date range\n",
    "# start_date = '2009-01-01'\n",
    "# end_date = '2016-06-30'\n",
    "# date_range = pd.date_range(start_date, end_date)\n",
    "# df_main = pd.DataFrame(index=date_range)\n",
    "\n",
    "# # Load SPY to get trading days\n",
    "# dfSPY = pd.read_csv('stock_data/SPY.csv', index_col='Date', parse_dates=True, usecols=['Date', 'Adj Close', 'Volume'], na_values = ['nan'])\n",
    "# dfSPY = dfSPY.rename(columns={'Adj Close': 'SPY', 'Volume': 'SPY_Vol'})\n",
    "# # Get SPY within the target date range\n",
    "# df_main = df_main.join(dfSPY)\n",
    "\n",
    "# # Drop NaN values\n",
    "# df_main = df_main.dropna()\n",
    "\n",
    "# # Load target stocks\n",
    "# dfSPY500_2009 = pd.read_csv('dev.csv', header=None, usecols = [1])\n",
    "# symbol = 'BF-B'\n",
    "# df_temp = pd.read_csv('stock_data/' + symbol + '.csv', index_col=\"Date\", parse_dates=True, usecols = ['Date', 'Volume', 'Adj Close'], na_values=['nan'])\n",
    "# df_temp = df_temp.rename(columns={'Volume': symbol + '_Vol', 'Adj Close': symbol})\n",
    "# df_main = df_main.join(df_temp, how='left')\n",
    "\n",
    "# # Load FSI data\n",
    "# dfFSI = pd.read_csv('STLFSI.csv', index_col='DATE', parse_dates=True, na_values = ['nan'])\n",
    "# df_main = df_main.join(dfFSI, how='left')\n",
    "\n",
    "display(df_main.head())\n",
    "print(df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we don't have any missing values to be taken care of for GOOGL. The financial stress index is calculated weekly so we'll have to fill in for the dates between the calculations ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A closer look\n",
    "Now let's take a look at the statistics of the stock price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'symbol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8658837e4664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_days\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmin_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmax_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmean_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmedian_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'symbol' is not defined"
     ]
    }
   ],
   "source": [
    "n_days = df_main.shape[0]\n",
    "min_price = df_main[symbol].min()\n",
    "max_price = df_main[symbol].max()\n",
    "mean_price = df_main[symbol].mean()\n",
    "median_price = np.median(df_main[symbol])\n",
    "std_price = df_main[symbol].std()\n",
    "cv = std_price / mean_price\n",
    "\n",
    "print(\"Number of traded days: {}\".format(n_days))\n",
    "print(\"Minimum stock price: {}\".format(min_price))\n",
    "print(\"Maximum stock price: {}\".format(max_price))\n",
    "print(\"Mean stock price: {}\".format(mean_price))\n",
    "print(\"Median stock price: {}\".format(median_price))\n",
    "print(\"Standard deviation of stock price: {}\".format(std_price))\n",
    "print(\"Coefficient of variation of stock price: {}\".format(cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we don't have any outliers here in the stock price data. And the coefficient of variation is less than 1. However, we can see the mean and median differ by quite a lot, so usually this is a good time to apply some non-linear feature scaling.\n",
    "\n",
    "Before we do that, let's inspect the statistics of other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(df_main.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the distribution of our data are generally skewed and the scales for different features are wildly differnt.\n",
    "\n",
    "To better see this, let's view our data in visual.\n",
    "\n",
    "### Exploratory Visualization\n",
    "To better see how our data are distributed and how they correlate, we can use a scatter matrix, with density plot in the diagonal.\n",
    "\n",
    "In this section, you will need to provide some form of visualization that summarizes or extracts a relevant characteristic or feature about the data. The visualization should adequately support the data being used. Discuss why this visualization was chosen and how it is relevant. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Have you visualized a relevant characteristic or feature about the dataset or input data?\n",
    "* Is the visualization thoroughly analyzed and discussed?\n",
    "* If a plot is provided, are the axes, title, and datum clearly defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# These are the \"Tableau 20\" colors as RGB.  \n",
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),  \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),  \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),  \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),  \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]  \n",
    "\n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.  \n",
    "for i in range(len(tableau20)):  \n",
    "    r, g, b = tableau20[i]  \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.) \n",
    "\n",
    "# pd.scatter_matrix(df_main, alpha = 0.3, figsize = (15,10), diagonal = 'kde', )\n",
    "g = sns.PairGrid(df_main)\n",
    "g.map_upper(plt.scatter, alpha=0.3)\n",
    "g.map_lower(plt.scatter, alpha=0.3)\n",
    "g.map_diag(sns.kdeplot, lw=3, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features in our dataset are not normally distributed, and the scales differ wildly. A common practice for sovling problem like this for financial data is to apply non-linear transformation to the data. Since some of our features have negative values, we can use a modified version of logorithm—signed logorithm to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "#### Algorithms\n",
    "In this project we will use the random forest algorithm to model our data. The random and ensemble nature of the algorithm makes it very unlikely to overfit on the training data. Furthermore, random forest is very easy to tune. We can easily grid search through the number of features for each tree and the number of trees. In addition to this, the ensemble nature of the algorithm makes it scalable when we need to 1. train on more data, 2. build more tres, 3. include more stocks to forecast. Overall, random forest generally gives good results and have been proven that ensemble algorithms like random forest perform over other traditional regression algorithms in the Kaggle community.\n",
    "\n",
    "#### Other techniques\n",
    "##### Training data with \"rolling training\"\n",
    "The part where stock marketing forecasting really differs from a lot of problems is that we are dealing with a highly time-dependent system. This means the data we collect for training is only valid within a range of time. In this project we will choose 100 days as our window size. For each data point we try to predict it's stock price by the model built only with the data in past 100 days, and move that window forward to predict for the next date. \n",
    "\n",
    "##### Dimension reduction\n",
    "As said above, this can also mean we wouldn't have a lot of data to train our model on. If this is the case, we would need to reduce our feature space with PCA so that we wouldn't suffer from the curse of dimensionality.\n",
    "\n",
    "### Benchmark\n",
    "I will use MSE as the benchmark for measuring performance, and when two models have very similar MSEs, I will use $r^2$ as the secondary benchmark. MSE can tell us how far off our predictions generally are. Since our goal is to predict the price as close to the real price as possible, MSE is a good choice for benchmark. MSE is generally sensative to outliers, however since we don't have any outliers in our dataset, this makes MSE even a better benchmark. The reason why we don't use $r^2$ is because some stock prices have higher variance. This can make $r^2$\n",
    "seems higher and does not give a good enough detail about the predicting power of our model.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "#### Treating missing values\n",
    "As discussed earlier, we have 1510 records missing for STLFSI due to how it is weekly calculated. We need to forward fill (to prevent the problem of peeking-into-the-future) and then back fill for days where the data is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Forward/Back Fill missing data\n",
    "df_main.fillna(method='ffill', inplace=True)\n",
    "df_main.fillna(method='bfill', inplace=True)\n",
    "\n",
    "## Display\n",
    "display(df_main.head(10))\n",
    "print(df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priliminary feature scaling and creating labels\n",
    "As discussed earlier, we can do a simple signed-log tranform to our data to bring all features to the same scale. The non-linear nature of the transform also helps normalize the data which is crucial for applying PCA (if need be).\n",
    "\n",
    "##### Create and separate out labels\n",
    "Since we don't need to apply feature scaling, we will separate it out first. Our goal is to predict prices 63 days from the day of prediction. We can use pandas shift method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split data\n",
    "data_y = df_main[symbol]\n",
    "\n",
    "def create_n_day_forecast_data(df, symbol, day):\n",
    "    df = df.shift(-day)\n",
    "    return df\n",
    "\n",
    "window = 63\n",
    "label_name = symbol + str(window) + 'd'\n",
    "\n",
    "data_y = create_n_day_forecast_data(data_y, symbol, window)\n",
    "data_y.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminary feature scaling\n",
    "The reader may have noticed we are keeping the price data. This is because we will need it for the feature engineering stage, thus we are keeping it and also scaling it along with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Signed-log transform\n",
    "log_data = np.sign(df_main) * np.log(abs(df_main) + 1)\n",
    "\n",
    "g = sns.PairGrid(log_data)\n",
    "g.map_upper(plt.scatter, alpha=0.2)\n",
    "g.map_lower(plt.scatter, alpha=0.2)\n",
    "g.map_diag(sns.kdeplot, lw=3, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the trends have become more clear. Not only can we see very clean-cut relationships between GOOGL and SPY but also good correlations between stock prices and stock volumes, and prices and the STLFSI.\n",
    "\n",
    "This lays a good foundation for us to do further feature engineering for our data.\n",
    "\n",
    "#### Feature engineering\n",
    "In this section we add a few features and remove some raw ones. The following are the complete feature list:\n",
    "\n",
    "* **STLFSI: **St. Louis Fed Financial Stress Index data (foward filled) \n",
    "* **Beta (63 days): **$\\beta = \\frac{Cov(r_a, r_b)}{Var(r_b)}$\n",
    "* **EMA (100 days): **$EMA_{today} = EMA_{yesterday} + \\alpha \\times (price_{today} - EMA_{yesterday}) \\ \\text{where }\\alpha = \\frac{2}{N+1}$\n",
    "* **MMA: ** $MMA_{today} = \\frac{(N-1) \\times MMA_{yesterday} + price}{N}$\n",
    "* **SMA (100 days): ** $SMA_{today} = \\frac{\\Sigma\\ prices}{\\text{number of days}}$\n",
    "* **Price Momentum (100 days): ** $\\frac{Momentum}{N + 1} = SMA_{today} - SMA_{yesterday}$\n",
    "* **SP500 SMA Change (100 days): ** $SMA_{today} - SMA_{yesterday} = \\frac{P_M - P_{M-n}}{n} \\ \\text{where in this case n = 100}$\n",
    "* **SP500 Volatility (63 days): ** $Std(r_{SP})$\n",
    "* **Sharpe Ratio (63 days): ** $\\text{Sharpe Ratio} = \\frac {r_a - r_{SP}}{\\sigma_{r_a}}$\n",
    "* **Volatility (63 days): ** $\\sigma_{r_a}$\n",
    "* **Volume Momentum (100 days): ** $\\frac{\\text{Volume Momentum}}{N + 1} = Volume_{today} - Volume_{yesterday}$\n",
    "* **Volume Marker 1: ** \n",
    "$\n",
    "              \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  1, \\ \\text{if Volume Momentum} \\geqslant 0\\\\\n",
    "                  0, \\ \\text{otherwise}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$\n",
    "* **Volume Marker 2: **\n",
    "$\n",
    "              \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  1, \\ \\text{if Volume is greater than the mean + standard deviation}\\\\\n",
    "                  0, \\ \\text{otherwise}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$\n",
    "* **Volume Marker 3: **\n",
    "$\n",
    "              \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  1, \\ \\text{if Volume is greater than the mean - standard deviation}\\\\\n",
    "                  0, \\ \\text{otherwise}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Feature Engineering Section\n",
    "symbols = ['SPY']\n",
    "symbols.append(symbol)\n",
    "\n",
    "### Make Daily Return Columns\n",
    "def compute_daily_returns(df, adj_close_name):\n",
    "    return (df / df.shift(1) - 1)[adj_close_name]\n",
    "\n",
    "for symbol in symbols:\n",
    "    log_data[symbol + '_return'] = compute_daily_returns(log_data, symbol)\n",
    "\n",
    "### Make Beta columns (63 days)\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "\n",
    "for symbol in symbols:\n",
    "    mean_dict[symbol] = []\n",
    "    std_dict[symbol] = []\n",
    "    \n",
    "    for i in log_data.index:\n",
    "        (u,) = log_data.index.get_indexer_for([i])\n",
    "        if u - 63 >= 0:\n",
    "            mean_dict[symbol].append(log_data[symbol + '_return'].iloc[u - 62:u+1].mean())\n",
    "            std_dict[symbol].append(log_data[symbol + '_return'].iloc[u - 62:u+1].std())\n",
    "        else:\n",
    "            mean_dict[symbol].append(np.nan)\n",
    "            std_dict[symbol].append(np.nan)\n",
    "    \n",
    "    log_data[symbol + '_Mean63d'] = mean_dict[symbol]\n",
    "    log_data[symbol + '_Std63d'] = std_dict[symbol]\n",
    "\n",
    "cov_dict = {}\n",
    "\n",
    "for symbol in symbols:\n",
    "    cov_dict[symbol] = []\n",
    "    for i in log_data.index:\n",
    "        (u,) = log_data.index.get_indexer_for([i])\n",
    "        if u - 62 >= 0:\n",
    "            cov_dict[symbol].append(log_data['SPY_return'].iloc[(u - 62):u+1].cov(log_data[symbol + '_return'].iloc[(u - 62):u+1]))\n",
    "        else:\n",
    "            cov_dict[symbol].append(np.nan)\n",
    "    log_data[symbol + '_Cov63d'] = cov_dict[symbol]\n",
    "    log_data[symbol + '_Beta'] = log_data[symbol + '_Cov63d'] / log_data[symbol + '_Std63d']**2\n",
    "\n",
    "### Make EMA column (100 days)\n",
    "EMA_dict = {}\n",
    "alpha = 2 / (100 + 1)\n",
    "\n",
    "for symbol in symbols:\n",
    "    EMA_dict[symbol] = []\n",
    "    EMA_dict[symbol].append(df_main[symbol].iloc[0])\n",
    "    \n",
    "    for i in log_data.index[1:]:\n",
    "        (u,) = log_data.index.get_indexer_for([i])\n",
    "        EMA_dict[symbol].append(EMA_dict[symbol][u - 1] + alpha * (log_data[symbol].iloc[u] - EMA_dict[symbol][u - 1]))\n",
    "\n",
    "    log_data[symbol + '_EMA'] = log_data[symbol]\n",
    "\n",
    "### Make MMA column (100 days)\n",
    "MMA_dict = {}\n",
    "alpha = 1 / 100\n",
    "\n",
    "for symbol in symbols:\n",
    "    MMA_dict[symbol] = []\n",
    "    MMA_dict[symbol].append(log_data[symbol].iloc[0])\n",
    "    \n",
    "    for i in log_data.index[1:]:\n",
    "        (u,) = log_data.index.get_indexer_for([i])\n",
    "        MMA_dict[symbol].append(MMA_dict[symbol][u - 1] + alpha * (log_data[symbol].iloc[u] - MMA_dict[symbol][u - 1]))\n",
    "\n",
    "    log_data[symbol + '_MMA'] = MMA_dict[symbol]\n",
    "    \n",
    "### Make SMA column (100 days)\n",
    "for symbol in symbols:\n",
    "    log_data[symbol + '_SMA'] = log_data[symbol].rolling(window=101, center=False).mean()\n",
    "\n",
    "### SMA Momentum\n",
    "def compute_SMA_momentum(df, SMA_column):\n",
    "    return (df - df.shift(1))[SMA_column]*(100 + 1)\n",
    "\n",
    "for symbol in symbols:\n",
    "    log_data[symbol + '_SMA_Momentum'] = compute_SMA_momentum(log_data, symbol + '_SMA')\n",
    "    \n",
    "### Volume Momentum\n",
    "def compute_Volume_momentum(df, Volume_column):\n",
    "    return (df - df.shift(1))[Volume_column]*(100 + 1)\n",
    "\n",
    "log_data[symbol + '_Vol_Momentum'] = compute_Volume_momentum(log_data, symbol + '_Vol')\n",
    "\n",
    "### Vol_Momentum Marker 1\n",
    "log_data[symbol + '_Vol_M1'] = np.nan\n",
    "log_data.loc[log_data[symbol + '_Vol_Momentum'] >= 0, symbol + '_Vol_M1'] = 1\n",
    "log_data.loc[log_data[symbol + '_Vol_Momentum'] < 0, symbol + '_Vol_M1'] = 0\n",
    "\n",
    "### Vol_Momentum Marker 2\n",
    "log_data[symbol + '_Vol_M2'] = np.nan\n",
    "log_data.loc[log_data[symbol + '_Vol'] >= (log_data[symbol + '_Vol'].rolling(window=101, center=False).mean() + log_data[symbol + '_Vol'].rolling(window=101, center=False).std()), symbol + '_Vol_M2'] = 1\n",
    "log_data.loc[log_data[symbol + '_Vol'] < (log_data[symbol + '_Vol'].rolling(window=101, center=False).mean() + log_data[symbol + '_Vol'].rolling(window=101, center=False).std()), symbol + '_Vol_M2'] = 0\n",
    "\n",
    "### Vol_Momentum Marker 3\n",
    "log_data[symbol + '_Vol_M3'] = np.nan\n",
    "log_data.loc[log_data[symbol + '_Vol'] < (log_data[symbol + '_Vol'].rolling(window=101, center=False).mean() - log_data[symbol + '_Vol'].rolling(window=101, center=False).std()), symbol + '_Vol_M3'] = 0\n",
    "log_data.loc[log_data[symbol + '_Vol'] >= (log_data[symbol + '_Vol'].rolling(window=101, center=False).mean() - log_data[symbol + '_Vol'].rolling(window=101, center=False).std()), symbol + '_Vol_M3'] = 1\n",
    "\n",
    "### Make SR column\n",
    "for symbol in symbols:\n",
    "    log_data[symbol + '_SR63d'] = log_data[symbol + '_return'].rolling(window=63, center=False).mean() / log_data[symbol + '_Std63d']\n",
    "\n",
    "### Drop not used SPY columns    \n",
    "SPY_keeper = log_data[['SPY_SMA_Momentum', 'SPY_Std63d']]\n",
    "\n",
    "for column in log_data.columns:\n",
    "    if 'SPY' in column:\n",
    "        log_data.drop([column], axis=1, inplace=True)\n",
    "\n",
    "### Put back SPY keeper\n",
    "log_data = pd.concat([log_data, SPY_keeper], axis=1)\n",
    "\n",
    "### Drop not used columns\n",
    "log_data.drop([symbol, symbol + '_return', symbol + '_Mean63d', symbol + '_Cov63d', symbol + '_Vol'], axis=1, inplace=True)\n",
    "\n",
    "print(log_data.columns)\n",
    "\n",
    "### Drop NaN rows\n",
    "log_data.dropna(inplace=True)\n",
    "\n",
    "### Normaliaze the features set match label index with the features set\n",
    "data_X = (log_data - log_data.mean()) / log_data.std()\n",
    "data_y = data_y.ix[data_X.index]\n",
    "data_y.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "In this section we will train our model and backtest it with the metrics we have defined earlier.\n",
    "\n",
    "#### Rolling Training\n",
    "As mentioned earlier, due to the nature of time series data, we will use a different method than our usual way with cross-sectional data. As said in the earlier section, we will choose 100 days as our window size. For each data point we try to predict its stock price by the model built only with the data in past 100 trading days, and move that window forward to predict for the next date. Note that our label is not the stock price of the day of predition, but 63 trading days from that day.\n",
    "\n",
    "#### Random Forest Default Parameters\n",
    "To start with we will use the default parameters given in the Python sklearn package:\n",
    "* Number of trees: 10\n",
    "* Max number of features in each tree: 14 (size of the feature space)\n",
    "* Boostrap samples are used for building trees\n",
    "\n",
    "Note that due to the stochastic nature of the algorithm, we set a random state (=10) to make sure the reproducibility of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Supervised Learning\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def make_predictions(data_X, data_y, date_range, estimator, window=100):\n",
    "    pred_y_list = []\n",
    "#     feature_importance_list = []\n",
    "    \n",
    "    for date in date_range:\n",
    "        test_X = data_X.ix[date]\n",
    "        (u,) = data_X.index.get_indexer_for([date])\n",
    "        \n",
    "        if u - window < 0:\n",
    "            raise ValueError(\"Not enough training data!\")\n",
    "            \n",
    "        train_X = data_X.iloc[(u - window):u]\n",
    "        train_y = data_y.iloc[(u - window):u]\n",
    "\n",
    "        estimator.fit(train_X, train_y)\n",
    "        pred_y = estimator.predict(test_X.reshape(1, -1))\n",
    "        pred_y_list.append(pred_y)\n",
    "#         feature_importance_list.append(estimator.feature_importances_)\n",
    "    \n",
    "#     vif = [int(np.argmax(x)) for x in feature_importance_list]\n",
    "#     vif = pd.DataFrame(vif)\n",
    "#     d = np.diff(np.unique(vif)).min()\n",
    "#     left_of_first_bin = vif.min() - float(d)/2\n",
    "#     right_of_last_bin = vif.max() + float(d)/2\n",
    "#     vif.plot.hist(alpha=0.5, bins=np.arange(left_of_first_bin, right_of_last_bin + d, d))\n",
    "    return pred_y_list\n",
    "\n",
    "# lr = SVR()\n",
    "lr = RandomForestRegressor(criterion='mse', random_state=10, bootstrap=True)\n",
    "reg = lr\n",
    "# parameters = {'n_estimators': [10, 100, 1000, 2000], 'max_features': ['sqrt', 'log2', 'auto']}\n",
    "# parameters = {'n_estimators': [10], 'max_features': ['sqrt', 'log2', 'auto']}\n",
    "# reg = grid_search.GridSearchCV(lr, parameters, cv=9, scoring='mean_squared_error')\n",
    "\n",
    "test_y = data_y[100:]\n",
    "pred_y = pd.Series(make_predictions(data_X, data_y, data_y.index[100:], reg, 100))\n",
    "pred_y = pd.DataFrame(pred_y.tolist(), index=test_y.index, columns=['Predict'])\n",
    "\n",
    "\n",
    "# # print(\"MSE: {}\".format(reg.score(test_X, test_y)))\n",
    "print(\"R^2: {}\".format(r2_score(test_y, pred_y)))\n",
    "print(\"MSE: {}\".format(mean_squared_error(test_y, pred_y)))\n",
    "\n",
    "dfResult2 = pred_y.join(test_y, )\n",
    "dfResult2.columns = ['Predict', label_name]\n",
    "\n",
    "dfResult2.plot(figsize=(12, 6))\n",
    "# dfResult2.ix[datetime(2014, 2, 24):datetime(2014, 3, 10)].plot(figsize=(12, 6))\n",
    "\n",
    "diff2 = (dfResult2['Predict'] - dfResult2[label_name]).to_frame()\n",
    "diff2 = diff2.join(dfResult2[label_name])\n",
    "diff2.columns = ['err', label_name]\n",
    "\n",
    "diff2.plot.scatter(x=label_name, y='err', figsize=(12, 6))\n",
    "plt.axhline(y=diff2['err'].mean())\n",
    "plt.show()\n",
    "\n",
    "print(\"Err mean: {}\".format(diff2['err'].mean()))\n",
    "print(\"2 * Std mean: {}\".format(2 * diff2['err'].std()))\n",
    "\n",
    "diff3 = ((dfResult2['Predict'] - dfResult2[label_name]) / dfResult2[label_name]).to_frame()\n",
    "diff3 = diff3.join(dfResult2[label_name])\n",
    "diff3.columns = ['err percentage', label_name]\n",
    "\n",
    "diff3.plot.scatter(x=label_name, y='err percentage', figsize=(12, 6))\n",
    "plt.axhline(y=diff3['err percentage'].mean())\n",
    "plt.show()\n",
    "\n",
    "print(\"Err percentage mean: {}\".format(diff3['err percentage'].mean()))\n",
    "print(\"2 * Std mean: {}\".format(2 * diff3['err percentage'].std()))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfResult2.ix[datetime(2012, 5, 1):datetime(2012, 5, 15)].plot(figsize=(12, 6))\n",
    "\n",
    "diff3 = ((dfResult2['Predict'] - dfResult2[label_name]) / dfResult2[label_name]).to_frame()\n",
    "diff3 = diff3.join(dfResult2[label_name])\n",
    "diff3.columns = ['err percentage', label_name]\n",
    "\n",
    "diff3.plot.scatter(x=label_name, y='err percentage', figsize=(12, 6))\n",
    "plt.axhline(y=diff3['err percentage'].mean())\n",
    "plt.show()\n",
    "\n",
    "print(\"Err percentage mean: {}\".format(diff3['err percentage'].mean()))\n",
    "print(\"2 * Std mean: {}\".format(2 * diff3['err percentage'].std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dfSPY500_2009[1].tolist())\n",
    "np.random.seed(0)\n",
    "print(np.random.choice(dfSPY500_2009[1].tolist(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "#### Grid Search and Cross Validation\n",
    "\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Has an initial solution been found and clearly reported?\n",
    "* Is the process of improvement clearly documented, such as what techniques were used?\n",
    "* Are intermediate and final solutions clearly reported as the process is improved?\n",
    "\n",
    "## Results\n",
    "(approximately 2 - 3 pages)\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "\n",
    "* Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?\n",
    "* Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?\n",
    "* Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?\n",
    "* Can results found from the model be trusted?\n",
    "\n",
    "### Justification\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Are the final results found stronger than the benchmark result reported earlier?\n",
    "* Have you thoroughly analyzed and discussed the final solution?\n",
    "* Is the final solution significant enough to have solved the problem?\n",
    "\n",
    "## Conclusion\n",
    "(approximately 1 - 2 pages)\n",
    "\n",
    "### Free-Form Visualization\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Have you visualized a relevant or important quality about the problem, dataset, input data, or results?\n",
    "* Is the visualization thoroughly analyzed and discussed?\n",
    "* If a plot is provided, are the axes, title, and datum clearly defined?\n",
    "\n",
    "### Reflection\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Have you thoroughly summarized the entire process you used for this project?\n",
    "* Were there any interesting aspects of the project?\n",
    "* Were there any difficult aspects of the project?\n",
    "* Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?\n",
    "\n",
    "### Improvement\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Are there further improvements that could be made on the algorithms or techniques you used in this project?\n",
    "* Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?\n",
    "* If you used your final solution as the new benchmark, do you think an even better solution exists?\n",
    "\n",
    "## Before submitting your report, ask yourself…\n",
    "* Does the project report you’ve written follow a well-organized structure similar to that of the project template?\n",
    "* Is each section (particularly Analysis and Methodology) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?\n",
    "* Would the intended audience of your project be able to understand your analysis, methods, and results?\n",
    "* Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?\n",
    "* Are all the resources used for this project correctly cited and referenced?\n",
    "* Is the code that implements your solution easily readable and properly commented?\n",
    "* Does the code execute without error and produce results similar to those reported?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "\n",
    "# # Core\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from datetime import date, datetime, timedelta\n",
    "\n",
    "# # Data Visualization\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display\n",
    "\n",
    "# # Supervised Learning\n",
    "# from sklearn import grid_search\n",
    "# from sklearn.metrics import r2_score\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Unsupervised Learning / PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Misc\n",
    "# from os import getcwd\n",
    "# import operator\n",
    "# from math import floor\n",
    "\n",
    "# # Page Configuration\n",
    "# pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# # Define date range\n",
    "# start_date = '2009-01-01'\n",
    "# end_date = '2016-06-30'\n",
    "# date_range = pd.date_range(start_date, end_date)\n",
    "# df_main = pd.DataFrame(index=date_range)\n",
    "\n",
    "# # Load SPY to get trading days\n",
    "# dfSPY = pd.read_csv('stock_data/SPY.csv', index_col='Date', parse_dates=True, usecols=['Date', 'Adj Close'], na_values = ['nan'])\n",
    "# dfSPY = dfSPY.rename(columns={'Adj Close': 'SPY'})\n",
    "# # Get SPY within the target date range\n",
    "# df_main = df_main.join(dfSPY)\n",
    "\n",
    "# # Drop NaN values\n",
    "# df_main = df_main.dropna()\n",
    "\n",
    "# # Load target stocks\n",
    "# dfSPY500_2009 = pd.read_csv('dev.csv', header=None, usecols = [1])\n",
    "# symbol = 'GOOGL'\n",
    "\n",
    "# df_temp = pd.read_csv('stock_data/' + symbol + '.csv', index_col=\"Date\", parse_dates=True, usecols = ['Date', 'Volume', 'Adj Close'], na_values=['nan'])\n",
    "# df_temp = df_temp.rename(columns={'Volume': symbol + '_Vol', 'Adj Close': symbol})\n",
    "\n",
    "# ### Forward/Back Fill missing data\n",
    "# df_main = df_main.join(df_temp, how='left')\n",
    "# df_main.fillna(method='ffill', inplace=True)\n",
    "# df_main.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    \n",
    "# ###\n",
    "# ### Feature Engineering Section\n",
    "# ###\n",
    "# symbols = ['SPY']\n",
    "# symbols.append(symbol)\n",
    "\n",
    "# ### Make Average Run-up Columns (252 days)\n",
    "\n",
    "# # temp_dict = {}\n",
    "\n",
    "# # for symbol in symbols:\n",
    "# #     temp_dict[symbol] = []\n",
    "    \n",
    "# #     for i in df_main.index:\n",
    "# #         (u,) = df_main.index.get_indexer_for([i])\n",
    "# #         if u - 252 >= 0:\n",
    "# #             temp_dict[symbol].append((df_main[symbol].iloc[u] - df_main[symbol].iloc[u - 252]) / 252)       \n",
    "# #         else:\n",
    "# #             temp_dict[symbol].append(np.nan)\n",
    "            \n",
    "# #     df_main[symbol + '_Avg_Runup'] = temp_dict[symbol]\n",
    "\n",
    "# ### Make Daily Return Columns\n",
    "# def compute_daily_returns(df, adj_close_name):\n",
    "#     return (df / df.shift(1) - 1)[adj_close_name]\n",
    "\n",
    "# for symbol in symbols:\n",
    "#     df_main[symbol + '_return'] = compute_daily_returns(df_main, symbol)\n",
    "\n",
    "# ### Make Beta columns (63 days)\n",
    "# mean_dict = {}\n",
    "# std_dict = {}\n",
    "\n",
    "# for symbol in symbols:\n",
    "#     mean_dict[symbol] = []\n",
    "#     std_dict[symbol] = []\n",
    "    \n",
    "#     for i in df_main.index:\n",
    "#         (u,) = df_main.index.get_indexer_for([i])\n",
    "#         if u - 63 >= 0:\n",
    "#             mean_dict[symbol].append(df_main[symbol + '_return'].iloc[u - 62:u+1].mean())\n",
    "#             std_dict[symbol].append(df_main[symbol + '_return'].iloc[u - 62:u+1].std())\n",
    "#         else:\n",
    "#             mean_dict[symbol].append(np.nan)\n",
    "#             std_dict[symbol].append(np.nan)\n",
    "    \n",
    "#     df_main[symbol + '_Mean63d'] = mean_dict[symbol]\n",
    "#     df_main[symbol + '_Std63d'] = std_dict[symbol]\n",
    "\n",
    "# cov_dict = {}\n",
    "\n",
    "# for symbol in symbols:\n",
    "#     cov_dict[symbol] = []\n",
    "#     for i in df_main.index:\n",
    "#         (u,) = df_main.index.get_indexer_for([i])\n",
    "#         if u - 62 >= 0:\n",
    "#             cov_dict[symbol].append(df_main['SPY_return'].iloc[(u - 62):u+1].cov(df_main[symbol + '_return'].iloc[(u - 62):u+1]))\n",
    "#         else:\n",
    "#             cov_dict[symbol].append(np.nan)\n",
    "#     df_main[symbol + '_Cov63d'] = cov_dict[symbol]\n",
    "#     df_main[symbol + '_Beta'] = df_main[symbol + '_Cov63d'] / df_main[symbol + '_Std63d']**2\n",
    "\n",
    "# ### Make EMA column (100 days)\n",
    "# EMA_dict = {}\n",
    "# alpha = 2 / (100 + 1)\n",
    "\n",
    "# for symbol in symbols:\n",
    "#     EMA_dict[symbol] = []\n",
    "#     EMA_dict[symbol].append(df_main[symbol].iloc[0])\n",
    "    \n",
    "#     for i in df_main.index[1:]:\n",
    "#         (u,) = df_main.index.get_indexer_for([i])\n",
    "#         EMA_dict[symbol].append(EMA_dict[symbol][u - 1] + alpha * (df_main[symbol].iloc[u] - EMA_dict[symbol][u - 1]))\n",
    "\n",
    "#     df_main[symbol + '_EMA'] = EMA_dict[symbol]\n",
    "\n",
    "# ### Make MMA column (100 days)\n",
    "# MMA_dict = {}\n",
    "# alpha = 1 / 100\n",
    "\n",
    "# for symbol in symbols:\n",
    "#     MMA_dict[symbol] = []\n",
    "#     MMA_dict[symbol].append(df_main[symbol].iloc[0])\n",
    "    \n",
    "#     for i in df_main.index[1:]:\n",
    "#         (u,) = df_main.index.get_indexer_for([i])\n",
    "#         MMA_dict[symbol].append(MMA_dict[symbol][u - 1] + alpha * (df_main[symbol].iloc[u] - MMA_dict[symbol][u - 1]))\n",
    "\n",
    "#     df_main[symbol + '_MMA'] = MMA_dict[symbol]\n",
    "\n",
    "# ### Make SMA column (100 days)\n",
    "# for symbol in symbols:\n",
    "#     df_main[symbol + '_SMA'] = df_main[symbol].rolling(window=101, center=False).mean()\n",
    "\n",
    "# ### SMA Momentum\n",
    "# def compute_SMA_momentum(df, SMA_column):\n",
    "#     return (df - df.shift(1))[SMA_column]*(100 + 1)\n",
    "\n",
    "# for symbol in symbols:\n",
    "#     df_main[symbol + '_SMA_Momentum'] = compute_SMA_momentum(df_main, symbol + '_SMA')\n",
    "    \n",
    "# ### Volume Momentum\n",
    "# def compute_Volume_momentum(df, Volume_column):\n",
    "#     return (df - df.shift(1))[Volume_column]*(100 + 1)\n",
    "\n",
    "# df_main[symbol + '_Vol_Momentum'] = compute_Volume_momentum(df_main, symbol + '_Vol')\n",
    "\n",
    "# ### Momentum Real 1\n",
    "# df_main[symbol + '_p_real1'] = np.nan\n",
    "# df_main.loc[df_main[symbol + '_Vol_Momentum'] >= 0, symbol + '_p_real1'] = 1\n",
    "# df_main.loc[df_main[symbol + '_Vol_Momentum'] < 0, symbol + '_p_real1'] = 0\n",
    "\n",
    "# ### Momentum Real 2\n",
    "# df_main[symbol + '_p_real2'] = np.nan\n",
    "# df_main.loc[df_main[symbol + '_Vol'] >= (df_main[symbol + '_Vol'].mean() + df_main[symbol + '_Vol'].std()), symbol + '_p_real2'] = 1\n",
    "# df_main.loc[df_main[symbol + '_Vol'] < (df_main[symbol + '_Vol'].mean() + df_main[symbol + '_Vol'].std()), symbol + '_p_real2'] = 0\n",
    "\n",
    "# # # Momentum Real 3\n",
    "# # df_main[symbol + '_p_real3'] = np.nan\n",
    "# # df_main.loc[df_main[symbol + '_Vol'] < (df_main[symbol + '_Vol'].mean() - df_main[symbol + '_Vol'].std()), symbol + '_p_real3'] = 0\n",
    "# # df_main.loc[df_main[symbol + '_Vol'] >= (df_main[symbol + '_Vol'].mean() - df_main[symbol + '_Vol'].std()), symbol + '_p_real3'] = 1\n",
    "\n",
    "# ### Make SR column\n",
    "# for symbol in symbols:\n",
    "#     df_main[symbol + '_SR63d'] = df_main[symbol + '_return'].rolling(window=63, center=False).mean() / df_main[symbol + '_Std63d']\n",
    "\n",
    "# # Load FSI data\n",
    "# dfFSI = pd.read_csv('STLFSI.csv', index_col='DATE', parse_dates=True, na_values = ['nan'])\n",
    "# df_main = df_main.join(dfFSI)\n",
    "# df_main['STLFSI'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# df_main[symbol + '5d'] = df_main.shift(-5)[symbol]\n",
    "# df_main[symbol + '10d'] = df_main.shift(-10)[symbol]\n",
    "# df_main[symbol + '15d'] = df_main.shift(-15)[symbol]\n",
    "# df_main[symbol + '20d'] = df_main.shift(-20)[symbol]\n",
    "# # print(df_main[['GOOGL_return', 'GOOGL_SMA_Vol']])\n",
    "\n",
    "# ### Drop not-used columns\n",
    "# for column in df_main.columns:\n",
    "#     if 'SPY' in column:\n",
    "#         df_main.drop([column], axis=1, inplace=True)\n",
    "\n",
    "# df_main.drop([symbol + '20d', symbol + '10d', symbol + '15d'], axis=1, inplace=True)\n",
    "\n",
    "# ### Drop NaN rows\n",
    "# df_main.dropna(inplace=True)\n",
    "\n",
    "# ### Split Data\n",
    "# data_X = df_main.drop([symbol + '5d'], axis=1)\n",
    "\n",
    "# # Normalization\n",
    "# data_X = (data_X - data_X.mean()) / data_X.std()\n",
    "# data_y = df_main[symbol + '5d']\n",
    "# # y_mean = data_y.mean()\n",
    "# # y_std = data_y.std()\n",
    "# # data_y = (data_y - y_mean) / y_std\n",
    "\n",
    "# # Feature Generation\n",
    "# # poly = PolynomialFeatures(2)\n",
    "# # data_X2 = poly.fit_transform(data_X)\n",
    "# # # data_X2 = SelectKBest(chi2, k=2).fit_transform(data_X, data_y)\n",
    "# # data_X = pd.DataFrame(data_X2, index=data_X.index)\n",
    "\n",
    "# # data_X.drop([0], axis=1, inplace=True)\n",
    "# # print(data_X.head())\n",
    "# # print(data_y)\n",
    "\n",
    "# train_X = data_X.iloc[:int(floor(data_X.shape[0] * 0.6))]\n",
    "# train_y = data_y.iloc[:int(floor(data_y.shape[0] * 0.6))]\n",
    "# test_X = data_X.iloc[int(floor(data_X.shape[0] * 0.6)):]\n",
    "# test_y = data_y.iloc[int(floor(data_y.shape[0] * 0.6)):]\n",
    "\n",
    "# # pca = PCA(n_components = 30)\n",
    "# # pca.fit(data_X)\n",
    "# # reduced_data = pca.transform(data_X)\n",
    "# # reduced_data = pd.DataFrame(reduced_data)\n",
    "\n",
    "# # train_X = reduced_data.iloc[:int(floor(data_X.shape[0] * 0.6))]\n",
    "# # test_X = reduced_data.iloc[int(floor(data_X.shape[0] * 0.6)):]\n",
    "\n",
    "# # lr = LinearRegression()\n",
    "# # parameters = {}\n",
    "# lr = SVR()\n",
    "# parameters = {'C': [1, 10, 100, 1000], 'degree': [1, 3, 5, 10], 'kernel': ['linear', 'poly', 'rbf']}\n",
    "\n",
    "# reg = grid_search.GridSearchCV(lr, parameters, cv=9, scoring='mean_squared_error')\n",
    "\n",
    "# reg.fit(train_X, train_y)\n",
    "# pred_y = reg.predict(test_X)\n",
    "# print(reg.get_params())\n",
    "# print(\"MSE: {}\".format(reg.score(test_X, test_y)))\n",
    "# print(\"R^2: {}\".format(r2_score(test_y, pred_y)))\n",
    "\n",
    "# ### Checking residual plot\n",
    "# dfResult = pd.DataFrame(reg.predict(test_X), index=test_y.index, columns=['Predict']).join(test_y, )\n",
    "# dfResult.ix[datetime(2014, 1, 1):datetime(2014, 1, 20)].plot(figsize=(12, 6))\n",
    "# # dfResult['dummy'] = range(len(dfResult))\n",
    "# # print(dfResult)\n",
    "# diff = (dfResult['Predict'] - dfResult[symbol + '5d']).to_frame()\n",
    "# diff = diff.join(dfResult[symbol + '5d'])\n",
    "# diff.columns = ['err', symbol + '5d']\n",
    "\n",
    "# diff.plot.scatter(x=symbol + '5d', y='err', figsize=(12, 6))\n",
    "# plt.axhline(y=diff['err'].mean())\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Err mean: {}\".format(diff['err'].mean()))\n",
    "# print(\"2 * Std mean: {}\".format(2 * diff['err'].std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import cross_validation\n",
    "# from sklearn.learning_curve import learning_curve\n",
    "# from data_visualizer.cross_validation import plot_learning_curve\n",
    "\n",
    "# # Linear Regression Learning Curve\n",
    "# title = \"Learning Curves (LinearRegression)\"\n",
    "# plot_learning_curve(reg, title, train_X, train_y, cv=5, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from math import floor\n",
    "\n",
    "# # Classifiers\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Regressors\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Clustering/Dim Reduction\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Read in raw dataset\n",
    "# data = pd.read_csv('first.csv', index_col=0) # raw data\n",
    "\n",
    "# # Dataset X (feature)\n",
    "# # data_X = data.drop(['Symbol', 'Date', 'Adj Close', 'RiseOrFall'], axis=1) # drop labels for unsupervised learning\n",
    "# data_X = data.drop(['Symbol', 'Date', 'Adj Close', 'RiseOrFall', 'open_1d', 'high_1d', 'low_1d', 'volume_1d', 'adjclose_1d', \\\n",
    "#         'open_3d_mean', 'high_3d_mean', 'low_3d_mean', 'volume_3d_mean', 'adjclose_3d_mean', \\\n",
    "#         'open_3d_max', 'high_3d_max', 'low_3d_max', 'volume_3d_max', 'adjclose_3d_max', \\\n",
    "#         'open_3d_min', 'high_3d_min', 'low_3d_min', 'volume_3d_min', 'adjclose_3d_min', \\\n",
    "#         'open_3d_std', 'high_3d_std', 'low_3d_std', 'volume_3d_std', 'adjclose_3d_std', \\\n",
    "#         'open_5d_mean', 'high_5d_mean', 'low_5d_mean', 'volume_5d_mean', 'adjclose_5d_mean', \\\n",
    "#         'open_5d_max', 'high_5d_max', 'low_5d_max', 'adjclose_5d_max', 'volume_5d_max', \\\n",
    "#         'open_5d_min', 'high_5d_min', 'low_5d_min', 'adjclose_5d_min', 'volume_5d_min', \\\n",
    "#         'open_5d_std', 'high_5d_std', 'low_5d_std', 'adjclose_5d_std', 'volume_5d_std'], axis=1) # drop labels for unsupervised learning\n",
    "# print(data.columns)\n",
    "\n",
    "# # Normalization\n",
    "# data_X = (data_X - data_X.mean()) / data_X.std()\n",
    "\n",
    "# y_mean = data['Adj Close'].mean()\n",
    "# y_std = data['Adj Close'].std()\n",
    "# data_y = (data['Adj Close'] - y_mean) / y_std\n",
    "\n",
    "# train_X = data_X.iloc[:int(floor(data_X.shape[0] * 0.6))]\n",
    "# train_y = data_y.iloc[:int(floor(data_y.shape[0] * 0.6))]\n",
    "# test_X = data_X.iloc[int(floor(data_X.shape[0] * 0.6)):]\n",
    "# test_y = data_y.iloc[int(floor(data.shape[0] * 0.6)):]\n",
    "\n",
    "# pca = PCA(n_components = 2)\n",
    "# pca.fit(data_X)\n",
    "# reduced_data = pca.transform(data_X)\n",
    "# reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "# # display(reduced_data)\n",
    "# train_X = reduced_data.iloc[:int(floor(data_X.shape[0] * 0.6))]\n",
    "# test_X = reduced_data.iloc[int(floor(data_X.shape[0] * 0.6)):]\n",
    "\n",
    "# # display(train_X.describe())\n",
    "\n",
    "# reg = LinearRegression()\n",
    "# reg.fit(train_X, train_y)\n",
    "\n",
    "# # plt.plot(reg.predict(test_X), data.ix[int(floor(data_X.shape[0] * 0.6)):, 'Date'])\n",
    "# # plt.show()\n",
    "# print(reg.score(test_X, test_y))\n",
    "\n",
    "\n",
    "# label = test_y.reset_index()\n",
    "# del label['index']\n",
    "\n",
    "# pred_label = pd.DataFrame(reg.predict(test_X), columns=['Predicted Label'])\n",
    "# pred_date = data.ix[int(floor(data_X.shape[0] * 0.6)):, 'Date']\n",
    "# test = pd.Series(pred_date).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "\n",
    "# result = pd.concat([pred_label, test, label], axis=1, join='inner')\n",
    "\n",
    "# # print(result)\n",
    "\n",
    "# result['Date'] = result['Date'].apply(np.datetime64)\n",
    "\n",
    "# # print(result.describe())\n",
    "\n",
    "# plt.figure(1, figsize=(40,20))\n",
    "# plt.axis([np.datetime64('2014-01-01'), np.datetime64('2014-01-10'), 5, 5])\n",
    "# plt.subplot(211)\n",
    "# plt.plot(result['Date'], result['Adj Close'])\n",
    "\n",
    "# plt.subplot(212)\n",
    "# plt.plot(result['Date'], result['Predicted Label'])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.cluster import AffinityPropagation\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# import operator\n",
    "\n",
    "# scores = {}\n",
    "\n",
    "# for n in range(2, 11):\n",
    "#     clusterer = KMeans(n_clusters = n)\n",
    "#     clusterer.fit(reduced_data)\n",
    "\n",
    "#     # TODO: Predict the cluster for each data point\n",
    "#     preds = clusterer.predict(reduced_data)\n",
    "\n",
    "#     # TODO: Find the cluster centers\n",
    "#     centers = clusterer.cluster_centers_\n",
    "\n",
    "#     # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "#     labels = clusterer.labels_\n",
    "#     score = silhouette_score(reduced_data, labels, metric='euclidean')\n",
    "#     scores[n] = score\n",
    "    \n",
    "#     print \"Silhouette score with n = {}: {}\".format(n, score)\n",
    "\n",
    "# max_key = max(scores.keys(), key=(lambda k: scores[k])) \n",
    "\n",
    "# clusterer = KMeans(n_clusters = max_key)\n",
    "# clusterer.fit(reduced_data)\n",
    "\n",
    "# # TODO: Predict the cluster for each data point\n",
    "# preds = clusterer.predict(reduced_data)\n",
    "\n",
    "# # TODO: Find the cluster centers\n",
    "# centers = clusterer.cluster_centers_\n",
    "\n",
    "# # TODO: Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "# labels = clusterer.labels_\n",
    "# score = silhouette_score(reduced_data, labels, metric='euclidean')\n",
    "        \n",
    "# print \"Cluster centers: \\n{}\\n\".format(centers)\n",
    "# print \"Silhouette score (max) with n = {}: {}\".format(max_key, score)\n",
    "# centers = clstr.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import renders as rs\n",
    "# rs.cluster_results(reduced_data, preds, centers, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "## Machine Learning Engineer Nanodegree\n",
    "**Calvin Ku**\n",
    "\n",
    "**July 15th, 2016**\n",
    "\n",
    "## Definition\n",
    "(approximately 1 - 2 pages)\n",
    "\n",
    "### Project Overview\n",
    "In this section, look to provide a high-level overview of the project in layman’s terms. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Has an overview of the project been provided, such as the problem domain, project origin, and related datasets or input data?\n",
    "* Has enough background information been given so that an uninformed reader would understand the problem domain and following problem statement?\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "In this section, you will want to clearly define the problem that you are trying to solve, including the strategy (outline of tasks) you will use to achieve the desired solution. You should also thoroughly discuss what the intended solution will be for this problem. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Is the problem statement clearly defined? Will the reader understand what you are expecting to solve?\n",
    "* Have you thoroughly discussed how you will attempt to solve the problem?\n",
    "* Is an anticipated solution clearly defined? Will the reader understand what results you are looking for?\n",
    "\n",
    "### Metrics\n",
    "In this section, you will need to clearly define the metrics or calculations you will use to measure performance of a model or result in your project. These calculations and metrics should be justified based on the characteristics of the problem and problem domain. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Are the metrics you’ve chosen to measure the performance of your models clearly discussed and defined?\n",
    "* Have you provided reasonable justification for the metrics chosen based on the problem and solution?\n",
    "\n",
    "## Analysis\n",
    "(approximately 2 - 4 pages)\n",
    "\n",
    "### Data Exploration\n",
    "In this section, you will be expected to analyze the data you are using for the problem. This data can either be in the form of a dataset (or datasets), input data (or input files), or even an environment. The type of data should be thoroughly described and, if possible, have basic statistics and information presented (such as discussion of input features or defining characteristics about the input or environment). Any abnormalities or interesting qualities about the data that may need to be addressed have been identified (such as features that need to be transformed or the possibility of outliers). Questions to ask yourself when writing this section:\n",
    "\n",
    "* If a dataset is present for this problem, have you thoroughly discussed certain features about the dataset? Has a data sample been provided to the reader?\n",
    "* If a dataset is present for this problem, are statistics about the dataset calculated and reported? Have any relevant results from this calculation been discussed?\n",
    "* If a dataset is not present for this problem, has discussion been made about the input space or input data for your problem?\n",
    "* Are there any abnormalities or characteristics about the input space or dataset that need to be addressed? (categorical variables, missing values, outliers, etc.)\n",
    "\n",
    "### Exploratory Visualization\n",
    "In this section, you will need to provide some form of visualization that summarizes or extracts a relevant characteristic or feature about the data. The visualization should adequately support the data being used. Discuss why this visualization was chosen and how it is relevant. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Have you visualized a relevant characteristic or feature about the dataset or input data?\n",
    "* Is the visualization thoroughly analyzed and discussed?\n",
    "* If a plot is provided, are the axes, title, and datum clearly defined?\n",
    "\n",
    "### Algorithms and Techniques\n",
    "In this section, you will need to discuss the algorithms and techniques you intend to use for solving the problem. You should justify the use of each one based on the characteristics of the problem and the problem domain. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Are the algorithms you will use, including any default variables/parameters in the project clearly defined?\n",
    "* Are the techniques to be used thoroughly discussed and justified?\n",
    "* Is it made clear how the input data or datasets will be handled by the algorithms and techniques chosen?\n",
    "\n",
    "### Benchmark\n",
    "In this section, you will need to provide a clearly defined benchmark result or threshold for comparing across performances obtained by your solution. The reasoning behind the benchmark (in the case where it is not an established result) should be discussed. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Has some result or value been provided that acts as a benchmark for measuring performance?\n",
    "* Is it clear how this result or value was obtained (whether by data or by hypothesis)?\n",
    "\n",
    "## Methodology\n",
    "(approximately 3 - 5 pages)\n",
    "\n",
    "### Data Preprocessing\n",
    "In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:\n",
    "\n",
    "* If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?\n",
    "* Based on the Data Exploration section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?\n",
    "* If no preprocessing is needed, has it been made clear why?\n",
    "\n",
    "### Implementation\n",
    "In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?\n",
    "* Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?\n",
    "* Was there any part of the coding process (e.g., writing complicated functions) that should be documented?\n",
    "\n",
    "### Refinement\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Has an initial solution been found and clearly reported?\n",
    "* Is the process of improvement clearly documented, such as what techniques were used?\n",
    "* Are intermediate and final solutions clearly reported as the process is improved?\n",
    "\n",
    "## Results\n",
    "(approximately 2 - 3 pages)\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "\n",
    "* Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?\n",
    "* Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?\n",
    "* Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?\n",
    "* Can results found from the model be trusted?\n",
    "\n",
    "### Justification\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Are the final results found stronger than the benchmark result reported earlier?\n",
    "* Have you thoroughly analyzed and discussed the final solution?\n",
    "* Is the final solution significant enough to have solved the problem?\n",
    "\n",
    "## Conclusion\n",
    "(approximately 1 - 2 pages)\n",
    "\n",
    "### Free-Form Visualization\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Have you visualized a relevant or important quality about the problem, dataset, input data, or results?\n",
    "* Is the visualization thoroughly analyzed and discussed?\n",
    "* If a plot is provided, are the axes, title, and datum clearly defined?\n",
    "\n",
    "### Reflection\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Have you thoroughly summarized the entire process you used for this project?\n",
    "* Were there any interesting aspects of the project?\n",
    "* Were there any difficult aspects of the project?\n",
    "* Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?\n",
    "\n",
    "### Improvement\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "\n",
    "* Are there further improvements that could be made on the algorithms or techniques you used in this project?\n",
    "* Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?\n",
    "* If you used your final solution as the new benchmark, do you think an even better solution exists?\n",
    "\n",
    "## Before submitting your report, ask yourself…\n",
    "* Does the project report you’ve written follow a well-organized structure similar to that of the project template?\n",
    "* Is each section (particularly Analysis and Methodology) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?\n",
    "* Would the intended audience of your project be able to understand your analysis, methods, and results?\n",
    "* Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?\n",
    "* Are all the resources used for this project correctly cited and referenced?\n",
    "* Is the code that implements your solution easily readable and properly commented?\n",
    "* Does the code execute without error and produce results similar to those reported?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
